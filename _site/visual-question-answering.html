<h3 id="state-of-the-art-in-visual-question-answering">State Of The Art in Visual Question Answering</h3>

<h3 id="coco-visual-question-answering-vqa-real-images-10-open-ended">COCO Visual Question Answering (VQA) real images 1.0 open ended</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1606.01847v3.pdf">EMNLP 2016 Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/Cadene/vqa.pytorch">MCB 7 att.</a></p>
  </li>
  <li>
    <p>Last Update: 2019-02-24</p>
  </li>
</ul>

<h3 id="vqa-v2">VQA v2</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1708.02711v1.pdf">CVPR 2018 Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/peteanderson80/bottom-up-attention">Image features from bottom-up attention, adaptive K, ensemble</a></p>
  </li>
  <li>
    <p>Last Update: 2018-05-04</p>
  </li>
</ul>

<h3 id="coco-visual-question-answering-vqa-real-images-10-multiple-choice">COCO Visual Question Answering (VQA) real images 1.0 multiple choice</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1606.01847v3.pdf">EMNLP 2016 Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/Cadene/vqa.pytorch">MCB 7 att.</a></p>
  </li>
  <li>
    <p>Last Update: 2019-02-24</p>
  </li>
</ul>

<h3 id="coco-visual-question-answering-vqa-real-images-20-open-ended">COCO Visual Question Answering (VQA) real images 2.0 open ended</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.07998v3.pdf">CVPR 2018 Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/peteanderson80/bottom-up-attention">Up-Down</a></p>
  </li>
  <li>
    <p>Last Update: 2018-05-04</p>
  </li>
</ul>

<h3 id="coco-visual-question-answering-vqa-abstract-images-10-open-ended">COCO Visual Question Answering (VQA) abstract images 1.0 open ended</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1609.05600v2.pdf">CVPR 2017 Graph-Structured Representations for Visual Question Answering</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">Graph VQA</a></p>
  </li>
</ul>

<h3 id="coco-visual-question-answering-vqa-abstract-10-multiple-choice">COCO Visual Question Answering (VQA) abstract 1.0 multiple choice</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1609.05600v2.pdf">CVPR 2017 Graph-Structured Representations for Visual Question Answering</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">Graph VQA</a></p>
  </li>
</ul>

<h3 id="visual7w">Visual7W</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1611.09978v1.pdf">CVPR 2017 Modeling Relationships in Referential Expressions with Compositional Modular Networks</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/hengyuan-hu/bottom-up-attention-vqa">CMN</a></p>
  </li>
  <li>
    <p>Last Update: 2018-05-05</p>
  </li>
</ul>

<h3 id="visual-genome-subjects">Visual Genome (subjects)</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1611.09978v1.pdf">CVPR 2017 Modeling Relationships in Referential Expressions with Compositional Modular Networks</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/hengyuan-hu/bottom-up-attention-vqa">CMN</a></p>
  </li>
  <li>
    <p>Last Update: 2018-05-05</p>
  </li>
</ul>

<h3 id="visual-genome-pairs">Visual Genome (pairs)</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1611.09978v1.pdf">CVPR 2017 Modeling Relationships in Referential Expressions with Compositional Modular Networks</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/hengyuan-hu/bottom-up-attention-vqa">CMN</a></p>
  </li>
  <li>
    <p>Last Update: 2018-05-05</p>
  </li>
</ul>

<h3 id="vizwiz">VizWiz</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1708.02711v1.pdf">CVPR 2018 Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/peteanderson80/bottom-up-attention">Pythia v0.3</a></p>
  </li>
  <li>
    <p>Last Update: 2018-05-04</p>
  </li>
</ul>

