---
path: '/stuff/'
title: 'Language Modelling'
type: 'NATURAL_LANGUAGE_PROCESSING'

layout: nil
---

### State Of The Art in Language Modelling  
### 语言模型的最新进展   

### Penn Treebank (Word Level)

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

* 论文: [Preprint 2019 语言模型是非监督多任务学习器](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* 最佳方法和代码: [GPT-2](https://github.com/openai/gpt-2)

* 最近更新: 2019-05-31

### enwiki8

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

* 论文: [Preprint 2019 语言模型是非监督多任务学习器](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* 最佳方法和代码: [GPT-2](https://github.com/openai/gpt-2)

* 最近更新: 2019-05-31

### One Billion Word

* Paper: [ICLR 2019 Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860v2.pdf)

* Best Method and Code: [Transformer-XL Large](https://github.com/huggingface/pytorch-pretrained-BERT)

* Last Update: 2019-06-06

* 论文: [ICLR 2019 Transformer-XL: 超越固定长度上下文的注意力语言模型](https://arxiv.org/pdf/1901.02860v2.pdf)

* 最佳方法和代码: [Transformer-XL Large](https://github.com/huggingface/pytorch-pretrained-BERT)

* 最近更新: 2019-06-06

### WikiText-103

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

* 论文: [Preprint 2019 语言模型是非监督多任务学习器](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* 最佳方法和代码: [GPT-2](https://github.com/openai/gpt-2)

* 最近更新: 2019-05-31

### Text8

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

* 论文: [Preprint 2019 语言模型是非监督多任务学习器](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* 最佳方法和代码: [GPT-2](https://github.com/openai/gpt-2)

* 最近更新: 2019-05-31

### WikiText-2

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

* 论文: [Preprint 2019 语言模型是非监督多任务学习器](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* 最佳方法和代码: [GPT-2](https://github.com/openai/gpt-2)

* 最近更新: 2019-05-31

### Hutter Prize

* Paper: [ICLR 2019 Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860v2.pdf)

* Best Method and Code: [24-layer Transformer-XL](https://github.com/huggingface/pytorch-pretrained-BERT)

* Last Update: 2019-06-06

* 论文: [ICLR 2019 Transformer-XL: 超越固定长度上下文的注意力语言模型](https://arxiv.org/pdf/1901.02860v2.pdf)

* 最佳方法和代码: [24-layer Transformer-XL](https://github.com/huggingface/pytorch-pretrained-BERT)

* 最近更新: 2019-06-06

### Penn Treebank (Character Level)

* Paper: [ICLR 2019 Trellis Networks for Sequence Modeling](https://arxiv.org/pdf/1810.06682v2.pdf)

* Best Method and Code: [Trellis Network](https://github.com/locuslab/trellisnet)

* Last Update: 2019-04-11

* 论文: [ICLR 2019 用于序列建模的Trellis网络](https://arxiv.org/pdf/1810.06682v2.pdf)

* 最佳方法和代码: [Trellis Network](https://github.com/locuslab/trellisnet)

* 最近更新: 2019-04-11

### Sequential MNIST

* Paper: [ICLR 2019 Trellis Networks for Sequence Modeling](https://arxiv.org/pdf/1810.06682v2.pdf)

* Best Method and Code: [Trellis Network](https://github.com/locuslab/trellisnet)

* Last Update: 2019-04-11

* 论文: [ICLR 2019 用于序列建模的Trellis网络](https://arxiv.org/pdf/1810.06682v2.pdf)

* 最佳方法和代码: [Trellis Network](https://github.com/locuslab/trellisnet)

* 最近更新: 2019-04-11


