<h3 id="state-of-the-art-in-machine-translation">State Of The Art in Machine Translation</h3>

<h3 id="wmt2014-english-french">WMT2014 English-French</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1808.09381v2.pdf">EMNLP 2018 Understanding Back-Translation at Scale</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/pytorch/fairseq">Transformer Big + BT</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="wmt2014-english-german">WMT2014 English-German</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1901.10430v2.pdf">ICLR 2019 Pay Less Attention with Lightweight and Dynamic Convolutions</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/pytorch/fairseq">DynamicConv</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="iwslt2015-german-english">IWSLT2015 German-English</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1706.03762v5.pdf">NeurIPS 2017 Attention Is All You Need</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorflow/tensor2tensor">Transformer</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="wmt2016-english-romanian">WMT2016 English-Romanian</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1705.03122v3.pdf">ICML 2017 Convolutional Sequence to Sequence Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/ParlAI">ConvS2S BPE40k</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="iwslt2015-english-german">IWSLT2015 English-German</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1706.03762v5.pdf">NeurIPS 2017 Attention Is All You Need</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorflow/tensor2tensor">Transformer</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="wmt2015-english-german">WMT2015 English-German</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1610.10099v2.pdf"> Neural Machine Translation in Linear Time</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/paarthneekhara/byteNet-tensorflow">ByteNet</a></p>
  </li>
  <li>
    <p>Last Update: 2017-08-23</p>
  </li>
</ul>

<h3 id="wmt2016-german-english">WMT2016 German-English</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1606.02891v2.pdf">WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/rsennrich/wmt16-scripts">Attentional encoder-decoder + BPE</a></p>
  </li>
  <li>
    <p>Last Update: 2018-06-25</p>
  </li>
</ul>

<h3 id="wmt2016-english-german">WMT2016 English-German</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1606.02891v2.pdf">WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/rsennrich/wmt16-scripts">Attentional encoder-decoder + BPE</a></p>
  </li>
  <li>
    <p>Last Update: 2018-06-25</p>
  </li>
</ul>

<h3 id="wmt2016-english-russian">WMT2016 English-Russian</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1606.02891v2.pdf">WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/rsennrich/wmt16-scripts">Attentional encoder-decoder + BPE</a></p>
  </li>
  <li>
    <p>Last Update: 2018-06-25</p>
  </li>
</ul>

<h3 id="wmt2016-romanian-english">WMT2016 Romanian-English</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1901.07291v1.pdf"> Cross-lingual Language Model Pretraining</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/UnsupervisedMT">MLM pretraining</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="wmt-2017-latvian-english">WMT 2017 Latvian-English</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.08392v1.pdf"> Impact of Corpora Quality on Neural Machine Translation</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/M4t1ss/parallel-corpora-tools">Transformer trained on highly filtered data</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="wmt2014-german-english">WMT2014 German-English</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1802.06901v3.pdf">EMNLP 2018 Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/nyu-dl/dl4mt-nonauto">Denoising autoencoders (non-autoregressive)</a></p>
  </li>
  <li>
    <p>Last Update: 2018-11-27</p>
  </li>
</ul>

<h3 id="wmt2016-english-czech">WMT2016 English-Czech</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1606.02891v2.pdf">WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/rsennrich/wmt16-scripts">Attentional encoder-decoder + BPE</a></p>
  </li>
  <li>
    <p>Last Update: 2018-06-25</p>
  </li>
</ul>

<h3 id="wmt2016-czech-english">WMT2016 Czech-English</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1606.02891v2.pdf">WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/rsennrich/wmt16-scripts">Attentional encoder-decoder + BPE</a></p>
  </li>
  <li>
    <p>Last Update: 2018-06-25</p>
  </li>
</ul>

<h3 id="wmt-2017-english-latvian">WMT 2017 English-Latvian</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.08392v1.pdf"> Impact of Corpora Quality on Neural Machine Translation</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/M4t1ss/parallel-corpora-tools">Transformer trained on highly filtered data</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="iwslt2015-thai-english">IWSLT2015 Thai-English</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1606.07947v4.pdf">EMNLP 2016 Sequence-Level Knowledge Distillation</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/harvardnlp/seq2seq-attn">Seq-KD + Seq-Inter + Word-KD</a></p>
  </li>
  <li>
    <p>Last Update: 2017-01-02</p>
  </li>
</ul>

<h3 id="wmt2016-russian-english">WMT2016 Russian-English</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1606.02891v2.pdf">WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/rsennrich/wmt16-scripts">Attentional encoder-decoder + BPE</a></p>
  </li>
  <li>
    <p>Last Update: 2018-06-25</p>
  </li>
</ul>

<h3 id="wmt-2018-finnish-english">WMT 2018 Finnish-English</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.08392v1.pdf"> Impact of Corpora Quality on Neural Machine Translation</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/M4t1ss/parallel-corpora-tools">Transformer trained on highly filtered data</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="20news">20NEWS</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1508.04025v5.pdf">EMNLP 2015 Effective Approaches to Attention-based Neural Machine Translation</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/fairseq-py">12</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="wmt2014-french-english">WMT2014 French-English</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1809.01272v1.pdf">EMNLP 2018 Unsupervised Statistical Machine Translation</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/artetxem/vecmap">SMT + iterative backtranslation (unsupervised)</a></p>
  </li>
  <li>
    <p>Last Update: 2018-10-25</p>
  </li>
</ul>

<h3 id="wmt-2018-estonian-english">WMT 2018 Estonian-English</h3>

<ul>
  <li>
    <p>Paper: <a href="http://aclweb.org/anthology/W18-6423">WS 2018 Tilde’s Machine Translation Systems for WMT 2018</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/M4t1ss/parallel-corpora-tools">Multi-pass backtranslated adapted transformer</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="wmt-2018-english-estonian">WMT 2018 English-Estonian</h3>

<ul>
  <li>
    <p>Paper: <a href="http://aclweb.org/anthology/W18-6423">WS 2018 Tilde’s Machine Translation Systems for WMT 2018</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/M4t1ss/parallel-corpora-tools">Multi-pass backtranslated adapted transformer</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="accurat-balanced-test-corpus-for-under-resourced-languages-estonian-russian">ACCURAT balanced test corpus for under resourced languages Estonian-Russian</h3>

<ul>
  <li>
    <p>Paper: <a href="http://aclweb.org/anthology/L18-1595">LREC 2018 Training and Adapting Multilingual NMT for Less-resourced and Morphologically Rich Languages</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tilde-nlp/multilingual-nmt-data-prep">Multilingual Transformer</a></p>
  </li>
  <li>
    <p>Last Update: 2018-05-14</p>
  </li>
</ul>

<h3 id="accurat-balanced-test-corpus-for-under-resourced-languages-russian-estonian">ACCURAT balanced test corpus for under resourced languages Russian-Estonian</h3>

<ul>
  <li>
    <p>Paper: <a href="http://aclweb.org/anthology/L18-1595">LREC 2018 Training and Adapting Multilingual NMT for Less-resourced and Morphologically Rich Languages</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tilde-nlp/multilingual-nmt-data-prep">Multilingual Transformer</a></p>
  </li>
  <li>
    <p>Last Update: 2018-05-14</p>
  </li>
</ul>

<h3 id="wmt-2018-english-finnish">WMT 2018 English-Finnish</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.08392v1.pdf"> Impact of Corpora Quality on Neural Machine Translation</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/M4t1ss/parallel-corpora-tools">Transformer trained on highly filtered data</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="wmt2015-english-russian">WMT2015 English-Russian</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1508.07909v5.pdf">ACL 2016 Neural Machine Translation of Rare Words with Subword Units</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/fairseq-py">C2-50k Segmentation</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

