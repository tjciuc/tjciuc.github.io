<h3 id="state-of-the-art-in-language-modelling">State Of The Art in Language Modelling</h3>

<h3 id="penn-treebank-word-level">Penn Treebank (Word Level)</h3>

<ul>
  <li>
    <p>Paper: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">Preprint 2019 Language Models are Unsupervised Multitask Learners</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/openai/gpt-2">GPT-2</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-31</p>
  </li>
</ul>

<h3 id="enwiki8">enwiki8</h3>

<ul>
  <li>
    <p>Paper: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">Preprint 2019 Language Models are Unsupervised Multitask Learners</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/openai/gpt-2">GPT-2</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-31</p>
  </li>
</ul>

<h3 id="one-billion-word">One Billion Word</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1901.02860v2.pdf">ICLR 2019 Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/huggingface/pytorch-pretrained-BERT">Transformer-XL Large</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-06</p>
  </li>
</ul>

<h3 id="wikitext-103">WikiText-103</h3>

<ul>
  <li>
    <p>Paper: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">Preprint 2019 Language Models are Unsupervised Multitask Learners</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/openai/gpt-2">GPT-2</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-31</p>
  </li>
</ul>

<h3 id="text8">Text8</h3>

<ul>
  <li>
    <p>Paper: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">Preprint 2019 Language Models are Unsupervised Multitask Learners</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/openai/gpt-2">GPT-2</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-31</p>
  </li>
</ul>

<h3 id="wikitext-2">WikiText-2</h3>

<ul>
  <li>
    <p>Paper: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">Preprint 2019 Language Models are Unsupervised Multitask Learners</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/openai/gpt-2">GPT-2</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-31</p>
  </li>
</ul>

<h3 id="hutter-prize">Hutter Prize</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1901.02860v2.pdf">ICLR 2019 Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/huggingface/pytorch-pretrained-BERT">24-layer Transformer-XL</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-06</p>
  </li>
</ul>

<h3 id="penn-treebank-character-level">Penn Treebank (Character Level)</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.06682v2.pdf">ICLR 2019 Trellis Networks for Sequence Modeling</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/locuslab/trellisnet">Trellis Network</a></p>
  </li>
  <li>
    <p>Last Update: 2019-04-11</p>
  </li>
</ul>

<h3 id="sequential-mnist">Sequential MNIST</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.06682v2.pdf">ICLR 2019 Trellis Networks for Sequence Modeling</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/locuslab/trellisnet">Trellis Network</a></p>
  </li>
  <li>
    <p>Last Update: 2019-04-11</p>
  </li>
</ul>

