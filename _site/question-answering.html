<h3 id="state-of-the-art-in-question-answering">State Of The Art in Question Answering</h3>

<h3 id="squad11">SQuAD1.1</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.04805v1.pdf"> BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google-research/bert">BERT (ensemble)</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-31</p>
  </li>
</ul>

<h3 id="cnn--daily-mail">CNN / Daily Mail</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1703.02620v1.pdf"> Linguistic Knowledge as Memory for Recurrent Neural Networks</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">GA+MAGE (32)</a></p>
  </li>
</ul>

<h3 id="wikiqa">WikiQA</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.07847v3.pdf"> Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/vanzytay/WSDM2018_HyperQA">HyperQA</a></p>
  </li>
  <li>
    <p>Last Update: 2018-06-13</p>
  </li>
</ul>

<h3 id="babi">bAbi</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1606.04582v6.pdf"> Query-Reduction Networks for Question Answering</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/uwnlp/qrn">QRN</a></p>
  </li>
  <li>
    <p>Last Update: 2017-09-03</p>
  </li>
</ul>

<h3 id="childrens-book-test">Childrenâ€™s Book Test</h3>

<ul>
  <li>
    <p>Paper: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">Preprint 2019 Language Models are Unsupervised Multitask Learners</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/openai/gpt-2">GPT-2</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-31</p>
  </li>
</ul>

<h3 id="coqa">CoQA</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.04805v1.pdf"> BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google-research/bert">BERT Large Augmented (single model)</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-31</p>
  </li>
</ul>

<h3 id="qasent">QASent</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.06038v4.pdf"> Neural Variational Inference for Text Processing</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/carpedm20/variational-text-tensorflow">Attentive LSTM</a></p>
  </li>
  <li>
    <p>Last Update: 2016-08-10</p>
  </li>
</ul>

<h3 id="narrativeqa">NarrativeQA</h3>

<ul>
  <li>
    <p>Paper: <a href="http://aclweb.org/anthology/D18-1054">EMNLP 2018 Cut to the Chase: A Context Zoom-in Network for Reading Comprehension</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">ConZNet</a></p>
  </li>
</ul>

<h3 id="triviaqa">TriviaQA</h3>

<ul>
  <li>
    <p>Paper: <a href="http://aclweb.org/anthology/D18-1237">EMNLP 2018 MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">MemoReader</a></p>
  </li>
</ul>

<h3 id="yahoocqa">YahooCQA</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.07847v3.pdf"> Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/vanzytay/WSDM2018_HyperQA">HyperQA</a></p>
  </li>
  <li>
    <p>Last Update: 2018-06-13</p>
  </li>
</ul>

<h3 id="semevalcqa">SemEvalCQA</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.07847v3.pdf"> Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/vanzytay/WSDM2018_HyperQA">HyperQA</a></p>
  </li>
  <li>
    <p>Last Update: 2018-06-13</p>
  </li>
</ul>

<h3 id="wikihop">WikiHop</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1901.00603v2.pdf">ICLR 2019 Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">CFC</a></p>
  </li>
</ul>

<h3 id="ai2-kaggle-dataset">AI2 Kaggle Dataset</h3>

<ul>
  <li>
    <p>Paper: <a href="http://aclweb.org/anthology/K17-1009">CONLL 2017 Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">IR Baseline</a></p>
  </li>
</ul>

<h3 id="trecqa">TrecQA</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.07847v3.pdf"> Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/vanzytay/WSDM2018_HyperQA">HyperQA</a></p>
  </li>
  <li>
    <p>Last Update: 2018-06-13</p>
  </li>
</ul>

<h3 id="ms-marco">MS MARCO</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1901.02262v1.pdf"> Multi-style Generative Reading Comprehension</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">Masque Q&amp;A Style</a></p>
  </li>
</ul>

<h3 id="newsqa">NewsQA</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1811.04210v2.pdf">NeurIPS 2018 Densely Connected Attention Propagation for Reading Comprehension</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/vanzytay/NIPS2018_DECAPROP">DecaProp</a></p>
  </li>
  <li>
    <p>Last Update: 2019-03-10</p>
  </li>
</ul>

<h3 id="webquestions">WebQuestions</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1506.02075v1.pdf"> Large-scale Simple Question Answering with Memory Networks</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/ParlAI">Memory Networks (ensemble)</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="clicr">CliCR</h3>

<ul>
  <li>
    <p>Paper: <a href="http://aclweb.org/anthology/N18-1140">HLT 2018 CliCR: a Dataset of Clinical Case Reports for Machine Reading Comprehension</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">Gated-Attention Reader</a></p>
  </li>
</ul>

<h3 id="reverb">Reverb</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1404.4326v1.pdf"> Open Question Answering with Weakly Supervised Embedding Models</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">Weakly Supervised Embeddings</a></p>
  </li>
</ul>

<h3 id="mctest-500">MCTest-500</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1603.08884v1.pdf">ACL 2016 A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/Maluuba/mctest-model">Parallel-Hierarchical</a></p>
  </li>
  <li>
    <p>Last Update: 2018-11-20</p>
  </li>
</ul>

<h3 id="quac">QuAC</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.06683v3.pdf">ICLR 2019 FlowQA: Grasping Flow in History for Conversational Machine Comprehension</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/momohuang/FlowQA">FlowQA (single model)</a></p>
  </li>
  <li>
    <p>Last Update: 2018-11-22</p>
  </li>
</ul>

<h3 id="quora-question-pairs">Quora Question Pairs</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.04805v1.pdf"> BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google-research/bert">BERT (single model)</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-31</p>
  </li>
</ul>

<h3 id="complexquestions">COMPLEXQUESTIONS</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.04412v1.pdf">SEMEVAL 2017 Evaluating Semantic Parsing against a Simple Web-based Question Answering Model</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">WebQA</a></p>
  </li>
</ul>

<h3 id="simplequestions">SimpleQuestions</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1506.02075v1.pdf"> Large-scale Simple Question Answering with Memory Networks</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/ParlAI">Memory Networks (ensemble)</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="natural-questions">Natural Questions</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1901.08634v2.pdf"> A BERT Baseline for the Natural Questions</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">BERT-joint</a></p>
  </li>
</ul>

<h3 id="jd-product-question-answer">JD Product Question Answer</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1901.07696v2.pdf"> Product-Aware Answer Generation in E-Commerce Question-Answering</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/gsh199449/productqa">PAAG</a></p>
  </li>
  <li>
    <p>Last Update: 2019-01-24</p>
  </li>
</ul>

<h3 id="mctest-160">MCTest-160</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1603.08884v1.pdf">ACL 2016 A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/Maluuba/mctest-model">syntax, frame, coreference, and word embedding features</a></p>
  </li>
  <li>
    <p>Last Update: 2018-11-20</p>
  </li>
</ul>

