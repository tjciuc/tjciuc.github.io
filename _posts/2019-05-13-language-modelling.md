---
path: '/stuff/'
title: 'Language Modelling'
type: 'NATURAL_LANGUAGE_PROCESSING'

layout: nil
---

### State Of The Art in Language Modelling  

### Penn Treebank (Word Level)

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

### enwiki8

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

### One Billion Word

* Paper: [ICLR 2019 Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860v2.pdf)

* Best Method and Code: [Transformer-XL Large](https://github.com/huggingface/pytorch-pretrained-BERT)

* Last Update: 2019-06-06

### WikiText-103

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

### Text8

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

### WikiText-2

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

### Hutter Prize

* Paper: [ICLR 2019 Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860v2.pdf)

* Best Method and Code: [24-layer Transformer-XL](https://github.com/huggingface/pytorch-pretrained-BERT)

* Last Update: 2019-06-06

### Penn Treebank (Character Level)

* Paper: [ICLR 2019 Trellis Networks for Sequence Modeling](https://arxiv.org/pdf/1810.06682v2.pdf)

* Best Method and Code: [Trellis Network](https://github.com/locuslab/trellisnet)

* Last Update: 2019-04-11

### Sequential MNIST

* Paper: [ICLR 2019 Trellis Networks for Sequence Modeling](https://arxiv.org/pdf/1810.06682v2.pdf)

* Best Method and Code: [Trellis Network](https://github.com/locuslab/trellisnet)

* Last Update: 2019-04-11

