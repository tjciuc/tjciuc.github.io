<h3 id="state-of-the-art-in-atari-games">State Of The Art in Atari Games</h3>

<h3 id="atari-2600-montezumas-revenge">Atari 2600 Montezumaâ€™s Revenge</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.12894v1.pdf"> Exploration by Random Network Distillation</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/openai/random-network-distillation">RND</a></p>
  </li>
  <li>
    <p>Last Update: 2018-11-22</p>
  </li>
</ul>

<h3 id="atari-2600-venture">Atari 2600 Venture</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.12894v1.pdf"> Exploration by Random Network Distillation</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/openai/random-network-distillation">RND</a></p>
  </li>
  <li>
    <p>Last Update: 2018-11-22</p>
  </li>
</ul>

<h3 id="atari-2600-freeway">Atari 2600 Freeway</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1611.04717v3.pdf">NeurIPS 2017 #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/nhynes/abc">TRPO-hash</a></p>
  </li>
  <li>
    <p>Last Update: 2017-12-12</p>
  </li>
</ul>

<h3 id="atari-2600-frostbite">Atari 2600 Frostbite</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.06581v3.pdf"> Dueling Network Architectures for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/Horizon">Prior+Duel noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-03</p>
  </li>
</ul>

<h3 id="atari-2600-qbert">Atari 2600 Q*Bert</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-private-eye">Atari 2600 Private Eye</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.06887v1.pdf">ICML 2017 A Distributional Perspective on Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/NervanaSystems/coach">C51 noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-05</p>
  </li>
</ul>

<h3 id="atari-2600-gravitar">Atari 2600 Gravitar</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1810.12894v1.pdf"> Exploration by Random Network Distillation</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/openai/random-network-distillation">RND</a></p>
  </li>
  <li>
    <p>Last Update: 2018-11-22</p>
  </li>
</ul>

<h3 id="atari-2600-seaquest">Atari 2600 Seaquest</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.06887v1.pdf">ICML 2017 A Distributional Perspective on Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/NervanaSystems/coach">C51 noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-05</p>
  </li>
</ul>

<h3 id="atari-2600-space-invaders">Atari 2600 Space Invaders</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-enduro">Atari 2600 Enduro</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.06887v1.pdf">ICML 2017 A Distributional Perspective on Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/NervanaSystems/coach">C51 noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-05</p>
  </li>
</ul>

<h3 id="atari-2600-pong">Atari 2600 Pong</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1312.5602v1.pdf"> Playing Atari with Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorpack/tensorpack">DQN best</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="atari-2600-amidar">Atari 2600 Amidar</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-asterix">Atari 2600 Asterix</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.06887v1.pdf">ICML 2017 A Distributional Perspective on Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/NervanaSystems/coach">C51 noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-05</p>
  </li>
</ul>

<h3 id="atari-2600-breakout">Atari 2600 Breakout</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1602.04621v3.pdf">NeurIPS 2016 Deep Exploration via Bootstrapped DQN</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorflow/models/tree/master/research/deep_contextual_bandits">Bootstrapped DQN</a></p>
  </li>
  <li>
    <p>Last Update: 2018-07-23</p>
  </li>
</ul>

<h3 id="atari-2600-alien">Atari 2600 Alien</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-assault">Atari 2600 Assault</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-beam-rider">Atari 2600 Beam Rider</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-kangaroo">Atari 2600 Kangaroo</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.05952v4.pdf"> Prioritized Experience Replay</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">Prior noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-gopher">Atari 2600 Gopher</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-robotank">Atari 2600 Robotank</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1602.04621v3.pdf">NeurIPS 2016 Deep Exploration via Bootstrapped DQN</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorflow/models/tree/master/research/deep_contextual_bandits">Bootstrapped DQN</a></p>
  </li>
  <li>
    <p>Last Update: 2018-07-23</p>
  </li>
</ul>

<h3 id="atari-2600-bowling">Atari 2600 Bowling</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1602.07714v2.pdf">NeurIPS 2016 Learning values across many orders of magnitude</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">DDQN+Pop-Art noop</a></p>
  </li>
</ul>

<h3 id="atari-2600-up-and-down">Atari 2600 Up and Down</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1602.01783v2.pdf"> Asynchronous Methods for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorpack/tensorpack">A3C LSTM hs</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="atari-2600-tutankham">Atari 2600 Tutankham</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-time-pilot">Atari 2600 Time Pilot</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1602.01783v2.pdf"> Asynchronous Methods for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorpack/tensorpack">A3C LSTM hs</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="atari-2600-name-this-game">Atari 2600 Name This Game</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-boxing">Atari 2600 Boxing</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-asteroids">Atari 2600 Asteroids</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1602.01783v2.pdf"> Asynchronous Methods for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorpack/tensorpack">A3C LSTM hs</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="atari-2600-zaxxon">Atari 2600 Zaxxon</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1602.01783v2.pdf"> Asynchronous Methods for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorpack/tensorpack">A3C FF hs</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="atari-2600-ice-hockey">Atari 2600 Ice Hockey</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.05952v4.pdf"> Prioritized Experience Replay</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">Prior noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-tennis">Atari 2600 Tennis</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-road-runner">Atari 2600 Road Runner</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1602.01783v2.pdf"> Asynchronous Methods for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorpack/tensorpack">A3C LSTM hs</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="atari-2600-krull">Atari 2600 Krull</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.06581v3.pdf"> Dueling Network Architectures for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/Horizon">Duel noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-03</p>
  </li>
</ul>

<h3 id="atari-2600-demon-attack">Atari 2600 Demon Attack</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.06887v1.pdf">ICML 2017 A Distributional Perspective on Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/NervanaSystems/coach">C51 noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-05</p>
  </li>
</ul>

<h3 id="atari-2600-crazy-climber">Atari 2600 Crazy Climber</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.06887v1.pdf">ICML 2017 A Distributional Perspective on Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/NervanaSystems/coach">C51 noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-05</p>
  </li>
</ul>

<h3 id="atari-2600-centipede">Atari 2600 Centipede</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1602.07714v2.pdf">NeurIPS 2016 Learning values across many orders of magnitude</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">DDQN+Pop-Art noop</a></p>
  </li>
</ul>

<h3 id="atari-2600-river-raid">Atari 2600 River Raid</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.06581v3.pdf"> Dueling Network Architectures for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/Horizon">Duel noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-03</p>
  </li>
</ul>

<h3 id="atari-2600-wizard-of-wor">Atari 2600 Wizard of Wor</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-double-dunk">Atari 2600 Double Dunk</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.05952v4.pdf"> Prioritized Experience Replay</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">Prior noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-battle-zone">Atari 2600 Battle Zone</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-video-pinball">Atari 2600 Video Pinball</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.06887v1.pdf">ICML 2017 A Distributional Perspective on Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/NervanaSystems/coach">C51 noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-05</p>
  </li>
</ul>

<h3 id="atari-2600-bank-heist">Atari 2600 Bank Heist</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.06581v3.pdf"> Dueling Network Architectures for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/Horizon">Duel noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-03</p>
  </li>
</ul>

<h3 id="atari-2600-atlantis">Atari 2600 Atlantis</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1703.03864v2.pdf"> Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/openai/evolution-strategies-starter">ES FF (1 hour) noop</a></p>
  </li>
  <li>
    <p>Last Update: 2018-11-22</p>
  </li>
</ul>

<h3 id="atari-2600-star-gunner">Atari 2600 Star Gunner</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1602.01783v2.pdf"> Asynchronous Methods for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/tensorpack/tensorpack">A3C LSTM hs</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-07</p>
  </li>
</ul>

<h3 id="atari-2600-fishing-derby">Atari 2600 Fishing Derby</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.06581v3.pdf"> Dueling Network Architectures for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/Horizon">Duel noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-03</p>
  </li>
</ul>

<h3 id="atari-2600-chopper-command">Atari 2600 Chopper Command</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-kung-fu-master">Atari 2600 Kung-Fu Master</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-hero">Atari 2600 HERO</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1707.06887v1.pdf">ICML 2017 A Distributional Perspective on Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/NervanaSystems/coach">C51 noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-05</p>
  </li>
</ul>

<h3 id="atari-2600-ms-pacman">Atari 2600 Ms. Pacman</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.05952v4.pdf"> Prioritized Experience Replay</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">Prior noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-james-bond">Atari 2600 James Bond</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-berzerk">Atari 2600 Berzerk</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.06581v3.pdf"> Dueling Network Architectures for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/Horizon">Prior+Duel noop</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-03</p>
  </li>
</ul>

<h3 id="atari-2600-defender">Atari 2600 Defender</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-57">Atari-57</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1803.00933v1.pdf">ICLR 2018 Distributed Prioritized Experience Replay</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/belepi93/Ape-X">Ape-X</a></p>
  </li>
  <li>
    <p>Last Update: 2019-04-28</p>
  </li>
</ul>

<h3 id="atari-2600-solaris">Atari 2600 Solaris</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-phoenix">Atari 2600 Phoenix</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1511.06581v3.pdf"> Dueling Network Architectures for Deep Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/facebookresearch/Horizon">Prior+Duel hs</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-03</p>
  </li>
</ul>

<h3 id="atari-2600-pitfall">Atari 2600 Pitfall!</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-skiing">Atari 2600 Skiing</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-yars-revenge">Atari 2600 Yars Revenge</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

<h3 id="atari-2600-surround">Atari 2600 Surround</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1806.06923v1.pdf">ICML 2018 Implicit Quantile Networks for Distributional Reinforcement Learning</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/google/dopamine">IQN</a></p>
  </li>
  <li>
    <p>Last Update: 2019-06-08</p>
  </li>
</ul>

