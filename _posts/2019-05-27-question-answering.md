---
path: '/stuff/'
title: 'Question Answering'
type: 'COMPUTER_VISION'

layout: nil
---

### State Of The Art in Question Answering  

### SQuAD1.1

* Paper: [ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805v1.pdf)

* Best Method and Code: [BERT (ensemble)](https://github.com/google-research/bert)

* Last Update: 2019-05-31

### CNN / Daily Mail

* Paper: [ Linguistic Knowledge as Memory for Recurrent Neural Networks](https://arxiv.org/pdf/1703.02620v1.pdf)

* Best Method and Code: [GA+MAGE (32)]()

### WikiQA

* Paper: [ Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering](https://arxiv.org/pdf/1707.07847v3.pdf)

* Best Method and Code: [HyperQA](https://github.com/vanzytay/WSDM2018_HyperQA)

* Last Update: 2018-06-13

### bAbi

* Paper: [ Query-Reduction Networks for Question Answering](https://arxiv.org/pdf/1606.04582v6.pdf)

* Best Method and Code: [QRN](https://github.com/uwnlp/qrn)

* Last Update: 2017-09-03

### Children's Book Test

* Paper: [Preprint 2019 Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Best Method and Code: [GPT-2](https://github.com/openai/gpt-2)

* Last Update: 2019-05-31

### CoQA

* Paper: [ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805v1.pdf)

* Best Method and Code: [BERT Large Augmented (single model)](https://github.com/google-research/bert)

* Last Update: 2019-05-31

### QASent

* Paper: [ Neural Variational Inference for Text Processing](https://arxiv.org/pdf/1511.06038v4.pdf)

* Best Method and Code: [Attentive LSTM](https://github.com/carpedm20/variational-text-tensorflow)

* Last Update: 2016-08-10

### NarrativeQA

* Paper: [EMNLP 2018 Cut to the Chase: A Context Zoom-in Network for Reading Comprehension](http://aclweb.org/anthology/D18-1054)

* Best Method and Code: [ConZNet]()

### TriviaQA

* Paper: [EMNLP 2018 MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller](http://aclweb.org/anthology/D18-1237)

* Best Method and Code: [MemoReader]()

### YahooCQA

* Paper: [ Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering](https://arxiv.org/pdf/1707.07847v3.pdf)

* Best Method and Code: [HyperQA](https://github.com/vanzytay/WSDM2018_HyperQA)

* Last Update: 2018-06-13

### SemEvalCQA

* Paper: [ Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering](https://arxiv.org/pdf/1707.07847v3.pdf)

* Best Method and Code: [HyperQA](https://github.com/vanzytay/WSDM2018_HyperQA)

* Last Update: 2018-06-13

### WikiHop

* Paper: [ICLR 2019 Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering](https://arxiv.org/pdf/1901.00603v2.pdf)

* Best Method and Code: [CFC]()

### AI2 Kaggle Dataset

* Paper: [CONLL 2017 Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification](http://aclweb.org/anthology/K17-1009)

* Best Method and Code: [IR Baseline]()

### TrecQA

* Paper: [ Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering](https://arxiv.org/pdf/1707.07847v3.pdf)

* Best Method and Code: [HyperQA](https://github.com/vanzytay/WSDM2018_HyperQA)

* Last Update: 2018-06-13

### MS MARCO

* Paper: [ Multi-style Generative Reading Comprehension](https://arxiv.org/pdf/1901.02262v1.pdf)

* Best Method and Code: [Masque Q&A Style]()

### NewsQA

* Paper: [NeurIPS 2018 Densely Connected Attention Propagation for Reading Comprehension](https://arxiv.org/pdf/1811.04210v2.pdf)

* Best Method and Code: [DecaProp](https://github.com/vanzytay/NIPS2018_DECAPROP)

* Last Update: 2019-03-10

### WebQuestions

* Paper: [ Large-scale Simple Question Answering with Memory Networks](https://arxiv.org/pdf/1506.02075v1.pdf)

* Best Method and Code: [Memory Networks (ensemble)](https://github.com/facebookresearch/ParlAI)

* Last Update: 2019-06-07

### CliCR

* Paper: [HLT 2018 CliCR: a Dataset of Clinical Case Reports for Machine Reading Comprehension](http://aclweb.org/anthology/N18-1140)

* Best Method and Code: [Gated-Attention Reader]()

### Reverb

* Paper: [ Open Question Answering with Weakly Supervised Embedding Models](https://arxiv.org/pdf/1404.4326v1.pdf)

* Best Method and Code: [Weakly Supervised Embeddings]()

### MCTest-500

* Paper: [ACL 2016 A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data](https://arxiv.org/pdf/1603.08884v1.pdf)

* Best Method and Code: [Parallel-Hierarchical](https://github.com/Maluuba/mctest-model)

* Last Update: 2018-11-20

### QuAC

* Paper: [ICLR 2019 FlowQA: Grasping Flow in History for Conversational Machine Comprehension](https://arxiv.org/pdf/1810.06683v3.pdf)

* Best Method and Code: [FlowQA (single model)](https://github.com/momohuang/FlowQA)

* Last Update: 2018-11-22

### Quora Question Pairs

* Paper: [ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805v1.pdf)

* Best Method and Code: [BERT (single model)](https://github.com/google-research/bert)

* Last Update: 2019-05-31

### COMPLEXQUESTIONS

* Paper: [SEMEVAL 2017 Evaluating Semantic Parsing against a Simple Web-based Question Answering Model](https://arxiv.org/pdf/1707.04412v1.pdf)

* Best Method and Code: [WebQA]()

### SimpleQuestions

* Paper: [ Large-scale Simple Question Answering with Memory Networks](https://arxiv.org/pdf/1506.02075v1.pdf)

* Best Method and Code: [Memory Networks (ensemble)](https://github.com/facebookresearch/ParlAI)

* Last Update: 2019-06-07

### Natural Questions

* Paper: [ A BERT Baseline for the Natural Questions](https://arxiv.org/pdf/1901.08634v2.pdf)

* Best Method and Code: [BERT-joint]()

### JD Product Question Answer

* Paper: [ Product-Aware Answer Generation in E-Commerce Question-Answering](https://arxiv.org/pdf/1901.07696v2.pdf)

* Best Method and Code: [PAAG](https://github.com/gsh199449/productqa)

* Last Update: 2019-01-24

### MCTest-160

* Paper: [ACL 2016 A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data](https://arxiv.org/pdf/1603.08884v1.pdf)

* Best Method and Code: [syntax, frame, coreference, and word embedding features](https://github.com/Maluuba/mctest-model)

* Last Update: 2018-11-20

