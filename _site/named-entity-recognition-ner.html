<h3 id="state-of-the-art-in-named-entity-recognition-ner">State Of The Art in Named Entity Recognition NER</h3>

<h3 id="conll-2003-english">CoNLL 2003 (English)</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1903.07785v1.pdf"> Cloze-driven Pretraining of Self-attention Networks</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="">CNN Large + fine-tune</a></p>
  </li>
</ul>

<h3 id="ontonotes-v5-english">Ontonotes v5 (English)</h3>

<ul>
  <li>
    <p>Paper: <a href="http://aclweb.org/anthology/C18-1139">COLING 2018 Contextual String Embeddings for Sequence Labeling</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/zalandoresearch/flair">Flair embeddings</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-30</p>
  </li>
</ul>

<h3 id="jnlpba">JNLPBA</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1809.07950v1.pdf"> CollaboNet: collaboration of deep neural networks for biomedical named entity recognition</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/wonjininfo/CollaboNet">CollaboNet</a></p>
  </li>
  <li>
    <p>Last Update: 2019-02-05</p>
  </li>
</ul>

<h3 id="bc5cdr">BC5CDR</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1903.10676v1.pdf"> SciBERT: Pretrained Contextualized Embeddings for Scientific Text</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/allenai/scibert">SciBERT (SciVocab)</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-13</p>
  </li>
</ul>

<h3 id="long-tail-emerging-entities">Long-tail emerging entities</h3>

<ul>
  <li>
    <p>Paper: <a href="http://aclweb.org/anthology/C18-1139">COLING 2018 Contextual String Embeddings for Sequence Labeling</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/zalandoresearch/flair">Flair embeddings</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-30</p>
  </li>
</ul>

<h3 id="scierc">SciERC</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1903.10676v1.pdf"> SciBERT: Pretrained Contextualized Embeddings for Scientific Text</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/allenai/scibert">SciBERT (SciVocab)</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-13</p>
  </li>
</ul>

<h3 id="ncbi-disease">NCBI-disease</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1903.10676v1.pdf"> SciBERT: Pretrained Contextualized Embeddings for Scientific Text</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/allenai/scibert">SciBERT (Base Vocab)</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-13</p>
  </li>
</ul>

<h3 id="wetlab">WetLab</h3>

<ul>
  <li>
    <p>Paper: <a href="https://arxiv.org/pdf/1904.00585v2.pdf"> Using Similarity Measures to Select Pretraining Data for NER</a></p>
  </li>
  <li>
    <p>Best Method and Code: <a href="https://github.com/daixiangau/naacl2019-select-pretraining-data-for-ner">BiLSTM-CRF with ELMo</a></p>
  </li>
  <li>
    <p>Last Update: 2019-05-24</p>
  </li>
</ul>

