---
path: '/stuff/'
title: 'Machine Translation'
type: 'NATURAL_LANGUAGE_PROCESSING'

layout: nil
---

### State Of The Art in Machine Translation  
### 机器翻译的最新进展  

### WMT2014 English-French

* Paper: [EMNLP 2018 Understanding Back-Translation at Scale](https://arxiv.org/pdf/1808.09381v2.pdf)

* Best Method and Code: [Transformer Big + BT](https://github.com/pytorch/fairseq)

* Last Update: 2019-06-07

* 论文: [EMNLP 2018 理解大规模反向翻译](https://arxiv.org/pdf/1808.09381v2.pdf)

* 最佳方法和代码: [Transformer Big + BT](https://github.com/pytorch/fairseq)

* 最近更新: 2019-06-07

### WMT2014 English-German

* Paper: [ICLR 2019 Pay Less Attention with Lightweight and Dynamic Convolutions](https://arxiv.org/pdf/1901.10430v2.pdf)

* Best Method and Code: [DynamicConv](https://github.com/pytorch/fairseq)

* Last Update: 2019-06-07

* 论文: [ICLR 2019 通过轻量级和动态卷积减少注意力](https://arxiv.org/pdf/1901.10430v2.pdf)

* 最佳方法和代码: [DynamicConv](https://github.com/pytorch/fairseq)

* 最近更新: 2019-06-07

### IWSLT2015 German-English

* Paper: [NeurIPS 2017 Attention Is All You Need](https://arxiv.org/pdf/1706.03762v5.pdf)

* Best Method and Code: [Transformer](https://github.com/tensorflow/tensor2tensor)

* Last Update: 2019-06-08

* 论文: [NeurIPS 2017 只需注意力机制](https://arxiv.org/pdf/1706.03762v5.pdf)

* 最佳方法和代码: [Transformer](https://github.com/tensorflow/tensor2tensor)

* 最近更新: 2019-06-08

### WMT2016 English-Romanian

* Paper: [ICML 2017 Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122v3.pdf)

* Best Method and Code: [ConvS2S BPE40k](https://github.com/facebookresearch/ParlAI)

* Last Update: 2019-06-07

* 论文: [ICML 2017 卷积序列到序列学习](https://arxiv.org/pdf/1705.03122v3.pdf)

* 最佳方法和代码: [ConvS2S BPE40k](https://github.com/facebookresearch/ParlAI)

* 最近更新: 2019-06-07

### IWSLT2015 English-German

* Paper: [NeurIPS 2017 Attention Is All You Need](https://arxiv.org/pdf/1706.03762v5.pdf)

* Best Method and Code: [Transformer](https://github.com/tensorflow/tensor2tensor)

* Last Update: 2019-06-08

* 论文: [NeurIPS 2017 只需注意力机制](https://arxiv.org/pdf/1706.03762v5.pdf)

* 最佳方法和代码: [Transformer](https://github.com/tensorflow/tensor2tensor)

* 最近更新: 2019-06-08

### WMT2015 English-German

* Paper: [Neural Machine Translation in Linear Time](https://arxiv.org/pdf/1610.10099v2.pdf)

* Best Method and Code: [ByteNet](https://github.com/paarthneekhara/byteNet-tensorflow)

* Last Update: 2017-08-23

* 论文: [线性时间内的神经网络机器翻译](https://arxiv.org/pdf/1610.10099v2.pdf)

* 最佳方法和代码: [ByteNet](https://github.com/paarthneekhara/byteNet-tensorflow)

* 最近更新: 2017-08-23

### WMT2016 German-English

* Paper: [WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* Best Method and Code: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* Last Update: 2018-06-25

* 论文: [WS 2016 爱丁堡神经网络机器翻译系统 WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* 最佳方法和代码: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* 最近更新: 2018-06-25

### WMT2016 English-German

* Paper: [WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* Best Method and Code: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* Last Update: 2018-06-25

* 论文: [WS 2016 爱丁堡神经网络机器翻译系统 WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* 最佳方法和代码: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* 最近更新: 2018-06-25

### WMT2016 English-Russian

* Paper: [WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* Best Method and Code: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* Last Update: 2018-06-25

* 论文: [WS 2016 爱丁堡神经网络机器翻译系统 WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* 最佳方法和代码: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* 最近更新: 2018-06-25

### WMT2016 Romanian-English

* Paper: [Cross-lingual Language Model Pretraining](https://arxiv.org/pdf/1901.07291v1.pdf)

* Best Method and Code: [MLM pretraining](https://github.com/facebookresearch/UnsupervisedMT)

* Last Update: 2019-06-07

* 论文: [跨语言模型的预训练](https://arxiv.org/pdf/1901.07291v1.pdf)

* 最佳方法和代码: [MLM pretraining](https://github.com/facebookresearch/UnsupervisedMT)

* 最近更新: 2019-06-07

### WMT 2017 Latvian-English

* Paper: [Impact of Corpora Quality on Neural Machine Translation](https://arxiv.org/pdf/1810.08392v1.pdf)

* Best Method and Code: [Transformer trained on highly filtered data](https://github.com/M4t1ss/parallel-corpora-tools)

* Last Update: 2019-06-08

* 论文: [语料库质量对神经网络机器翻译的影响](https://arxiv.org/pdf/1810.08392v1.pdf)

* 最佳方法和代码: [Transformer trained on highly filtered data](https://github.com/M4t1ss/parallel-corpora-tools)

* 最近更新: 2019-06-08

### WMT2014 German-English

* Paper: [EMNLP 2018 Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement](https://arxiv.org/pdf/1802.06901v3.pdf)

* Best Method and Code: [Denoising autoencoders (non-autoregressive)](https://github.com/nyu-dl/dl4mt-nonauto)

* Last Update: 2018-11-27

* 论文: [EMNLP 2018 确定性非自回归神经序列模型的迭代细化](https://arxiv.org/pdf/1802.06901v3.pdf)

* 最佳方法和代码: [Denoising autoencoders (non-autoregressive)](https://github.com/nyu-dl/dl4mt-nonauto)

* 最近更新: 2018-11-27

### WMT2016 English-Czech

* Paper: [WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* Best Method and Code: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* Last Update: 2018-06-25

* 论文: [WS 2016 爱丁堡神经网络机器翻译系统 WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* 最佳方法和代码: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* 最近更新: 2018-06-25

### WMT2016 Czech-English

* Paper: [WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* Best Method and Code: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* Last Update: 2018-06-25

* 论文: [WS 2016 爱丁堡神经网络机器翻译系统 WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* 最佳方法和代码: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* 最近更新: 2018-06-25

### WMT 2017 English-Latvian

* Paper: [Impact of Corpora Quality on Neural Machine Translation](https://arxiv.org/pdf/1810.08392v1.pdf)

* Best Method and Code: [Transformer trained on highly filtered data](https://github.com/M4t1ss/parallel-corpora-tools)

* Last Update: 2019-06-08

* 论文: [语料库质量对神经网络机器翻译的影响](https://arxiv.org/pdf/1810.08392v1.pdf)

* 最佳方法和代码: [Transformer trained on highly filtered data](https://github.com/M4t1ss/parallel-corpora-tools)

* 最近更新: 2019-06-08

### IWSLT2015 Thai-English

* Paper: [EMNLP 2016 Sequence-Level Knowledge Distillation](https://arxiv.org/pdf/1606.07947v4.pdf)

* Best Method and Code: [Seq-KD + Seq-Inter + Word-KD](https://github.com/harvardnlp/seq2seq-attn)

* Last Update: 2017-01-02

* 论文: [EMNLP 2016 序列级别的知识提炼](https://arxiv.org/pdf/1606.07947v4.pdf)

* 最佳方法和代码: [Seq-KD + Seq-Inter + Word-KD](https://github.com/harvardnlp/seq2seq-attn)

* 最近更新: 2017-01-02

### WMT2016 Russian-English

* Paper: [WS 2016 Edinburgh Neural Machine Translation Systems for WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* Best Method and Code: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* Last Update: 2018-06-25

* 论文: [WS 2016 爱丁堡神经网络机器翻译系统 WMT 16](https://arxiv.org/pdf/1606.02891v2.pdf)

* 最佳方法和代码: [Attentional encoder-decoder + BPE](https://github.com/rsennrich/wmt16-scripts)

* 最近更新: 2018-06-25

### WMT 2018 Finnish-English

* Paper: [ Impact of Corpora Quality on Neural Machine Translation](https://arxiv.org/pdf/1810.08392v1.pdf)

* Best Method and Code: [Transformer trained on highly filtered data](https://github.com/M4t1ss/parallel-corpora-tools)

* Last Update: 2019-06-08

* 论文: [语料库质量对神经网络机器翻译的影响](https://arxiv.org/pdf/1810.08392v1.pdf)

* 最佳方法和代码: [Transformer trained on highly filtered data](https://github.com/M4t1ss/parallel-corpora-tools)

* 最近更新: 2019-06-08

### 20NEWS

* Paper: [EMNLP 2015 Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025v5.pdf)

* Best Method and Code: [12](https://github.com/facebookresearch/fairseq-py)

* Last Update: 2019-06-07

* 论文: [EMNLP 2015 高效的基于注意力的神经网络机器翻译](https://arxiv.org/pdf/1508.04025v5.pdf)

* 最佳方法和代码: [12](https://github.com/facebookresearch/fairseq-py)

* 最近更新: 2019-06-07

### WMT2014 French-English

* Paper: [EMNLP 2018 Unsupervised Statistical Machine Translation](https://arxiv.org/pdf/1809.01272v1.pdf)

* Best Method and Code: [SMT + iterative backtranslation (unsupervised)](https://github.com/artetxem/vecmap)

* Last Update: 2018-10-25

* 论文: [EMNLP 2018 无监督统计机器翻译](https://arxiv.org/pdf/1809.01272v1.pdf)

* 最佳方法和代码: [SMT + iterative backtranslation (unsupervised)](https://github.com/artetxem/vecmap)

* 最近更新: 2018-10-25

### WMT 2018 Estonian-English

* Paper: [WS 2018 Tilde's Machine Translation Systems for WMT 2018](http://aclweb.org/anthology/W18-6423)

* Best Method and Code: [Multi-pass backtranslated adapted transformer](https://github.com/M4t1ss/parallel-corpora-tools)

* Last Update: 2019-06-08

* 论文: [WS 2018 Tilde 机器翻译系统 WMT 2018](http://aclweb.org/anthology/W18-6423)

* 最佳方法和代码: [Multi-pass backtranslated adapted transformer](https://github.com/M4t1ss/parallel-corpora-tools)

* 最近更新: 2019-06-08

### WMT 2018 English-Estonian

* Paper: [WS 2018 Tilde's Machine Translation Systems for WMT 2018](http://aclweb.org/anthology/W18-6423)

* Best Method and Code: [Multi-pass backtranslated adapted transformer](https://github.com/M4t1ss/parallel-corpora-tools)

* Last Update: 2019-06-08

* 论文: [WS 2018 Tilde 机器翻译系统 WMT 2018](http://aclweb.org/anthology/W18-6423)

* 最佳方法和代码: [Multi-pass backtranslated adapted transformer](https://github.com/M4t1ss/parallel-corpora-tools)

* 最近更新: 2019-06-08

### ACCURAT balanced test corpus for under resourced languages Estonian-Russian

* Paper: [LREC 2018 Training and Adapting Multilingual NMT for Less-resourced and Morphologically Rich Languages](http://aclweb.org/anthology/L18-1595)

* Best Method and Code: [Multilingual Transformer](https://github.com/tilde-nlp/multilingual-nmt-data-prep)

* Last Update: 2018-05-14

* 论文: [LREC 2018 为低资源和多模态语言训练和调整多语言NMT模型](http://aclweb.org/anthology/L18-1595)

* 最佳方法和代码: [Multilingual Transformer](https://github.com/tilde-nlp/multilingual-nmt-data-prep)

* 最近更新: 2018-05-14

### ACCURAT balanced test corpus for under resourced languages Russian-Estonian

* Paper: [LREC 2018 Training and Adapting Multilingual NMT for Less-resourced and Morphologically Rich Languages](http://aclweb.org/anthology/L18-1595)

* Best Method and Code: [Multilingual Transformer](https://github.com/tilde-nlp/multilingual-nmt-data-prep)

* Last Update: 2018-05-14

* 论文: [LREC 2018 为低资源和多模态语言训练和调整多语言NMT模型](http://aclweb.org/anthology/L18-1595)

* 最佳方法和代码: [Multilingual Transformer](https://github.com/tilde-nlp/multilingual-nmt-data-prep)

* 最近更新: 2018-05-14

### WMT 2018 English-Finnish

* Paper: [Impact of Corpora Quality on Neural Machine Translation](https://arxiv.org/pdf/1810.08392v1.pdf)

* Best Method and Code: [Transformer trained on highly filtered data](https://github.com/M4t1ss/parallel-corpora-tools)

* Last Update: 2019-06-08

* 论文: [语料库质量对神经网络机器翻译的影响](https://arxiv.org/pdf/1810.08392v1.pdf)

* 最佳方法和代码: [Transformer trained on highly filtered data](https://github.com/M4t1ss/parallel-corpora-tools)

* 最近更新: 2019-06-08

### WMT2015 English-Russian

* Paper: [ACL 2016 Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909v5.pdf)

* Best Method and Code: [C2-50k Segmentation](https://github.com/facebookresearch/fairseq-py)

* Last Update: 2019-06-07

* 论文: [ACL 2016 对带子词单元生僻字的神经网络机器翻译](https://arxiv.org/pdf/1508.07909v5.pdf)

* 最佳方法和代码: [C2-50k Segmentation](https://github.com/facebookresearch/fairseq-py)

* 最近更新: 2019-06-07


